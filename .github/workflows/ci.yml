# .github/workflows/ci.yml
# Comprehensive CI/CD pipeline for Space Invaders JS project
# Combines Docker builds, testing, performance monitoring, and security scanning

name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

env:
  DOCKER_BUILDKIT: 1
  COMPOSE_DOCKER_CLI_BUILD: 1
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '20.x'
  PERFORMANCE_THRESHOLD: '1000'  # milliseconds
  MEMORY_THRESHOLD: '512'        # MB

jobs:
  validate:
    name: Validate Project Structure
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Validate file structure
        run: |
          required_files=(
            "src/config/constants.js"
            "src/core/Canvas.js"
            "src/core/GameLoop.js"
            "src/core/Renderer.js"
            "src/utils/Performance.js"
            "package.json"
            "Dockerfile"
          )
          for file in "${required_files[@]}"; do
            if [ ! -f "$file" ]; then
              echo "❌ Missing required file: $file"
              exit 1
            fi
          done
          echo "✅ File structure validation passed"

  docker-build:
    name: Docker Build
    needs: validate
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Build development image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: false
          tags: space-invaders:dev
          target: dev
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Build production image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: false
          tags: space-invaders:prod
          target: prod
          cache-from: type=gha
          cache-to: type=gha,mode=max

  test:
    name: Run Tests
    needs: docker-build
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run unit tests
        run: npm test

      - name: Run tests in Docker
        run: docker-compose -f docker-compose.test.yml up --build --exit-code-from test

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: |
            test-results/
            coverage/
          retention-days: 7

  performance:
    name: Performance Tests
    needs: test
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-benchmark memory_profiler

      - name: Run performance benchmarks
        id: benchmarks
        run: |
          python tests/performance/benchmark_3a2c2412-e1a1-455b-ad87-0514e8d942fe.py
        continue-on-error: true

      - name: Analyze performance results
        run: |
          echo "Analyzing performance metrics..."
          python -c "
          import json
          import sys
          
          try:
              with open('benchmark_results.json', 'r') as f:
                  results = json.load(f)
              
              # Extract performance metrics from results
              perf_results = results.get('results', [])
              if perf_results:
                  frame_times = [r.get('mean_ms', 0) for r in perf_results if 'frame' in r.get('name', '').lower()]
                  avg_frame_time = sum(frame_times) / len(frame_times) if frame_times else 0
              else:
                  avg_frame_time = 0
              
              # Check memory usage from system info
              memory_info = results.get('system_info', {}).get('memory', {})
              max_memory = memory_info.get('rss_mb', 0) or 0
              
              print(f'Average frame time: {avg_frame_time:.2f}ms')
              print(f'Memory usage: {max_memory:.2f}MB')
              
              if avg_frame_time > float('${{ env.PERFORMANCE_THRESHOLD }}'):
                  print(f'❌ Performance threshold exceeded: {avg_frame_time}ms > ${{ env.PERFORMANCE_THRESHOLD }}ms')
                  sys.exit(1)
              
              if max_memory > float('${{ env.MEMORY_THRESHOLD }}'):
                  print(f'❌ Memory threshold exceeded: {max_memory}MB > ${{ env.MEMORY_THRESHOLD }}MB')
                  sys.exit(1)
              
              print('✅ All performance metrics within acceptable thresholds')
          except Exception as e:
              print(f'Warning: Error analyzing results: {e}')
              print('Continuing without performance validation')
          "

      - name: Run E2E performance tests
        run: |
          if [ -f "tests/e2e/test_3a2c2412-e1a1-455b-ad87-0514e8d942fe_complete.py" ]; then
            python -m pytest tests/e2e/test_3a2c2412-e1a1-455b-ad87-0514e8d942fe_complete.py -v
          else
            echo "E2E test file not found, skipping"
          fi

      - name: Generate performance report
        if: always()
        run: |
          echo "## Performance Test Results" > performance_report.md
          echo "### Test Environment" >> performance_report.md
          echo "- OS: Ubuntu Latest" >> performance_report.md
          echo "- Python: ${{ env.PYTHON_VERSION }}" >> performance_report.md
          echo "- Node.js: ${{ env.NODE_VERSION }}" >> performance_report.md
          echo "### Metrics" >> performance_report.md
          
          if [ -f "benchmark_results.json" ]; then
            python -c "
            import json
            try:
                with open('benchmark_results.json', 'r') as f:
                    results = json.load(f)
                with open('performance_report.md', 'a') as report:
                    perf_results = results.get('results', [])
                    for result in perf_results:
                        report.write(f'\n- {result.get(\"name\", \"Unknown\")}: {result.get(\"mean_ms\", \"N/A\")}ms')
                    
                    memory_info = results.get('system_info', {}).get('memory', {})
                    if memory_info.get('rss_mb'):
                        report.write(f'\n- Memory Usage: {memory_info.get(\"rss_mb\", \"N/A\")}MB')
            except Exception as e:
                print(f'Error generating report: {e}')
            "
          else
            echo "- No benchmark results available" >> performance_report.md
          fi

      - name: Upload Performance Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: |
            benchmark_results.json
            performance_report.md
          retention-days: 14

  security:
    name: Security Scan
    needs: docker-build
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: 'space-invaders:prod'
          format: 'table'
          exit-code: '1'
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'

  deploy-preview:
    name: Deploy PR Preview
    needs: [test, performance]
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    permissions:
      contents: read
      pages: write
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Set GitHub Pages base path
        run: |
          echo "GITHUB_PAGES_BASE=/space-invaders-js-v100/pr-${{ github.event.number }}/" >> $GITHUB_ENV

      - name: Build for GitHub Pages
        run: |
          npm run build
        env:
          GITHUB_PAGES_BASE: /space-invaders-js-v100/pr-${{ github.event.number }}/

      - name: Create deployment directory
        run: |
          mkdir -p deploy/pr-${{ github.event.number }}
          cp -r dist/* deploy/pr-${{ github.event.number }}/

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload to GitHub Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./deploy

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

      - name: Output preview URL
        run: |
          echo "🚀 Preview URL: https://${{ github.repository_owner }}.github.io/space-invaders-js-v100/pr-${{ github.event.number }}/"
          echo "preview_url=https://${{ github.repository_owner }}.github.io/space-invaders-js-v100/pr-${{ github.event.number }}/" >> $GITHUB_OUTPUT

  deploy-main:
    name: Deploy to Production
    needs: [test, performance, security]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    permissions:
      contents: read
      pages: write
      id-token: write
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build for production
        run: npm run build
        env:
          GITHUB_PAGES_BASE: /space-invaders-js-v100/

      - name: Setup Pages
        uses: actions/configure-pages@v4

      - name: Upload to GitHub Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./dist

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

  notify:
    name: Notification
    needs: [test, performance, security, deploy-preview, deploy-main]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Check workflow status
        if: ${{ contains(needs.*.result, 'failure') }}
        run: exit 1

      - name: Send notification
        if: always()
        run: |
          echo "Pipeline completed with status:"
          echo "Test: ${{ needs.test.result }}"
          echo "Performance: ${{ needs.performance.result }}"
          echo "Security: ${{ needs.security.result }}"
          echo "Deploy Preview: ${{ needs.deploy-preview.result }}"
          echo "Deploy Main: ${{ needs.deploy-main.result }}"
